{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_mnist_gan_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/motkeg/Capital-market-learning-site/blob/master/fashion_mnist_gan_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "88fo6PERGgyH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fasion_MNIST GAN (Generative Adversarial Networks)"
      ]
    },
    {
      "metadata": {
        "id": "nyE-C8fuBAYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (Input, Dense, Reshape, Flatten, Dropout,\n",
        "                                    BatchNormalization, Activation, ZeroPadding2D,\n",
        "                                    LeakyReLU, UpSampling2D, Conv2D)\n",
        "\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "JOB_DIR = \"./weights/gan_v2\"    \n",
        "USE_TPU = False\n",
        "EPOCHS = 1000\n",
        "BATCH= 128\n",
        "SAVE = 100\n",
        "USE_TPU = False\n",
        "\n",
        "# Deterministic output.\n",
        "# Tired of seeing the same results every time? Remove the line below.\n",
        "np.random.seed(1000)\n",
        "if not os.path.exists(\"./samples/fashion_mnist_v2\"):\n",
        "    os.makedirs(\"./samples/fashion_mnist_v2\")\n",
        "    \n",
        "if not os.path.exists(JOB_DIR):\n",
        "    os.makedirs(JOB_DIR)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WxOMMlJiBM4L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        " # Plot the loss from each batch\n",
        "def plot_loss(epoch,d_Losses,g_Losses):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(d_Losses, label='Discriminitive loss')\n",
        "    plt.plot(d_Losses, label='Generative loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.savefig('dcgan_loss_epoch_%d.png' % epoch)\n",
        "\n",
        "# Create a wall of generated images\n",
        "def plot_generated_images(imgs , epoch, examples=25, dim=(5,5), figsize=(5, 5)):\n",
        "    noise = np.random.normal(0, 1, size=[examples, 100])\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(imgs.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(imgs[i, 0], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    name = \"fashion_mnist_v2_{}.png\".format(epoch)\n",
        "    plt.savefig('./samples/fashion_mnist_v2/' + name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pqFyzNX4GgIh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class DCGAN_V2():\n",
        "  \n",
        "    def __init__(self):\n",
        "        \n",
        "        # The results are a little better when the dimensionality of the random vector is only 10.\n",
        "        # The dimensionality has been left at 100 for consistency with other GAN implementations.\n",
        "        self.randomDim = 100\n",
        "\n",
        "        # Load data\n",
        "        (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "        X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
        "        #self.X_train = np.expand_dims(X_train, -1)\n",
        "        self.X_train = X_train[:, np.newaxis, :, :]\n",
        "\n",
        "        self.tensorboard = keras.callbacks.TensorBoard(log_dir=JOB_DIR,\n",
        "                                                        batch_size=BATCH,\n",
        "                                                        write_graph=True,\n",
        "                                                        histogram_freq=0,\n",
        "                                                        write_images=True,\n",
        "                                                        write_grads=True)\n",
        "        #self.checkpointer  = keras.callbacks.ModelCheckpoint(filepath=f'{FLAGS.job_dir}/gan_model.best.hdf5', verbose = 1, save_best_only=True)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
        "\n",
        "        self.discriminator = self.build_D()\n",
        "        self.generator = self.build_G()\n",
        "\n",
        "        # Combined network\n",
        "        self.discriminator.trainable = False\n",
        "        ganInput = Input(shape=(self.randomDim,))\n",
        "        x = self.generator(ganInput)\n",
        "        ganOutput = self.discriminator(x)\n",
        "        self.gan = Model(inputs=ganInput, outputs=ganOutput)\n",
        "        self.gan.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "\n",
        "        self.dLosses = []\n",
        "        self.gLosses = []\n",
        "\n",
        "\n",
        "    def build_G(self):\n",
        "        # Generator\n",
        "        generator = Sequential()\n",
        "        generator.add(Dense(128*7*7, input_dim=self.randomDim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "        generator.add(LeakyReLU(0.2))\n",
        "        generator.add(Reshape((128,7, 7)))\n",
        "        generator.add(UpSampling2D(size=(2, 2)))\n",
        "        generator.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
        "        generator.add(LeakyReLU(0.2))\n",
        "        generator.add(UpSampling2D(size=(2, 2)))\n",
        "        generator.add(Conv2D(1, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
        "        generator.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "        generator.summary()\n",
        "        return generator\n",
        "\n",
        "\n",
        "    def build_D(self):\n",
        "        # Discriminator\n",
        "        discriminator = Sequential()\n",
        "\n",
        "        discriminator.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(1,28, 28), kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
        "        discriminator.add(LeakyReLU(0.2))\n",
        "        discriminator.add(Dropout(0.3))\n",
        "        discriminator.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
        "        discriminator.add(LeakyReLU(0.2))\n",
        "        discriminator.add(Dropout(0.3))\n",
        "        discriminator.add(Flatten())\n",
        "        discriminator.add(Dense(1, activation='sigmoid'))\n",
        "        discriminator.compile(loss='binary_crossentropy', optimizer=self.optimizer)\n",
        "        discriminator.summary()\n",
        "        return discriminator\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def __call__(self, epochs=1, batchSize=BATCH):\n",
        "        batchCount = self.X_train.shape[0] // batchSize\n",
        "        print (f'Epochs:{epochs}\\nBatch size: {batchSize}\\t | Batches per epoch: {batchCount}')\n",
        "            \n",
        "        for e in range(1, epochs+1):\n",
        "            print ('-'*15, 'Epoch %d' % e, '-'*15)\n",
        "            for _ in tqdm(range(batchCount)):\n",
        "                # Get a random set of input noise and images\n",
        "                noise = np.random.normal( 0,1, size=[batchSize, self.randomDim])\n",
        "                imageBatch = self.X_train[np.random.randint(0, self.X_train.shape[0], size=batchSize)]\n",
        "\n",
        "                # Generate fake images\n",
        "                generatedImages = self.generator.predict(noise)\n",
        "                X = np.concatenate([imageBatch, generatedImages])\n",
        "\n",
        "                # Labels for generated and real data\n",
        "                yDis = np.zeros(2*batchSize)\n",
        "                # One-sided label smoothing\n",
        "                yDis[:batchSize] = 0.9\n",
        "\n",
        "                # Train discriminator\n",
        "                self.discriminator.trainable = True\n",
        "                dloss = self.discriminator.train_on_batch(X, yDis)\n",
        "\n",
        "                # Train generator\n",
        "                noise = np.random.normal(0, 1, size=[batchSize,self.randomDim])\n",
        "                yGen = np.ones(batchSize)\n",
        "                self.discriminator.trainable = False\n",
        "                gloss = self.gan.train_on_batch(noise, yGen)\n",
        "\n",
        "            # Store loss of most recent batch from this epoch\n",
        "            self.dLosses.append(dloss)\n",
        "            self.gLosses.append(gloss)\n",
        "\n",
        "            if e == 1 or e % 5 == 0:\n",
        "                noise = np.random.normal(0, 1, size=[25, self.randomDim])\n",
        "                imgs = self.generator.predict(noise)\n",
        "                plot_generated_images(imgs,e)\n",
        "                self.save_models(e)\n",
        "\n",
        "        # Plot losses from every epoch\n",
        "        plot_loss(e , self.dLosses,self.gLosses)\n",
        "\n",
        "    # Save the generator and discriminator networks (and weights) for later use\n",
        "    def save_models(self,epoch):\n",
        "        self.generator.save(f'{JOB_DIR}/dcgan_generator.h5')\n",
        "        self.discriminator.save(f'{JOB_DIR}/dcgan_discriminator.h5')\n",
        "        self.gan.save(f'{JOB_DIR}/dcgan_combined.h5')        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jPbMdi3pBTOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model = DCGAN_V2()\n",
        "model(epochs=EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBxMVINwjDQ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "7df5b6cb-33a6-4ebb-ce59-6db12593b1fb"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "LABEL_NAMES = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']\n",
        "\n",
        "PREDICT = 'trouser'\n",
        "\n",
        "\n",
        "# Create a wall of generated images\n",
        "def plot_images(imgs , dim=(5,5), figsize=(5, 5)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i in range(imgs.shape[0]):\n",
        "        plt.subplot(dim[0], dim[1], i+1)\n",
        "        plt.imshow(imgs[i, 0], interpolation='nearest', cmap='gray_r')\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "       \n",
        "                   \n",
        "label  = \" \"   \n",
        "runs =0\n",
        "fashion_cnn =  keras.models.Sequential()\n",
        "fashion_cnn.add(keras.layers.Reshape((28,28,1)))\n",
        "          \n",
        "generator  = load_model(\"dcgan_generator (1).h5\")\n",
        "model = load_model('fashion-cnn-weights.best.hdf5')\n",
        "fashion_cnn.add(model)\n",
        "\n",
        "#fashion_cnn = keras.models.Model(keras.layers.Reshape((28,28,1)), model.output)\n",
        "\n",
        "while label != PREDICT:\n",
        "  runs +=1\n",
        "  noise = np.random.normal(0, 1, size=[1, 100]) # generate one image \n",
        "  imgs = generator.predict(noise)\n",
        "  score  = fashion_cnn.predict(imgs) \n",
        "  indx  = list(score[0]).index(max(score[0]))\n",
        "  #print(score[0])\n",
        "  #print(indx)\n",
        "  label = LABEL_NAMES[indx]\n",
        "print(\"Runs:\",runs)  \n",
        "print(label)\n",
        "plot_images(imgs)  \n",
        "  \n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runs: 11\n",
            "trouser\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEwAAABMCAYAAADHl1ErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABXRJREFUeJztm8tOG0kUhr9yt902tA3honDJBSkr\nJDaR2EZCkfIAeYAs8xKRss8+4h3yAjwEG3YgFIRQonBRwAo22MZ2tyuLVh3azCymxj1KjVTfyqIb\nVP37P5c61Sittcbzjyn96QX83/CCWeIFs8QLZokXzBIvmCVeMEu8YJZ4wSwJ//QCHqK1Zn9/H4CP\nHz+yvLws1y4uLgBYWlqiVMq+66urKwDSNCVNUwBmZmY4PDwEYHNzk0+fPgEwPT098fqUa1sjrTU7\nOzsAvH//nqmpKQBGoxHdblc+J0kCQL1eB+Dm5oZKpQLAysoKrVYLgCAI2N3dBWBubm7i9TnnMKWU\nPNjS0hLVahXIRBoOh3KPEeru7g6AMAxpNBoAdLtd+RwEAXEcF7Y+n8Mscc5hANfX1wD0+31xSq/X\nw2SPMAzp9/sAlMtlIAvlZrMJQKPR4ObmBkDyWlE4Kdje3h6QCRYEAQBxHItg/X5fkn4YZo+Qpim1\nWg3IRBwMBkAWyuaeIvAhaYmTDvv16xcAtVpNWoHb21u5Xq/XJeQMpVJJwi8IAkajEQBRFKGUKmxt\nTgr28+dPIHvwTqcDZDnKtBhJkkgLYcJNKSVhqLWWUC6XyxLKRQjnQ9ISJx1mOvogCMQdURSR77Hz\nFROy5G5c12q1xGHmZ0XhpGDtdhvIHtaEkWlaIRPShJ9pbDudzlg1NEIlSSL5zFTWSfAhaYlzDtNa\nE0UR8FdXGbeZfSTcb41KpZJcz4fuYDCQv1NEP+acYEqpse4+n4MeCgGIuPmOPk1T5ufnAfjx44eI\nahrbSfAhaYlzDtNai3sAqXbD4VCa2GazKc4z7gHGQtaMgrTW0ss9evRo4vU5J1iapvR6PSAbBJq8\n87DCPX/+HICvX78CWfdvRFJKSfUEuLy8BODJkycTr8+HpCXOOez29lbGzk+fPpU95HA4FJclScLG\nxgYAx8fHQObA/KjHDA2jKOL79+8AvHz5cuL1OSdYq9WScArDcGykk28b1tfXAfjy5Yvca2ZkYRiO\ntRvmCygCH5KWOOewJEmk0QyCQPorpZS4rdFo8PjxY4Cxiprv2cx2qNVqyQS3CLzDLHHOYd1uV5J7\nHMecnZ0BmWNMXnrx4gWzs7PAfe8Vx7E4Mz9cDIJAZv1F4JxgvV5vrFk1TWe9XhdxFhcXZZhoKmOn\n05HfK5VKUl2jKJLtUxH4kLTEOYd9+/ZNXFOtVqXrX1hYkE4+fzhrDnRHo5GE6fX1tRSDwWBQyBzM\n4JxgJycnMr6pVCry4EopqYLlcll6NTPZuLy8lJ/Nz89L7js6OmJ7exuADx8+SNj+W3xIWuKcw66u\nrlhdXQWynsy4LYoi+ZwkifRkZu51dnYm1xcXF8VhAK9fvwaY2F3goGDtdlserFqtSoXLn4JPTU1J\nC2FyWaVSkfA8ODjg8+fPALx586bQHOZD0hLnHAb3vVWtVpNXn/IFIE1T2fqYKpk/7e71erx69Qoo\n5qQoj3OClctlEWlmZoaFhQVg/JAD7veK5nqSJFIlZ2dnpds3DW5R+JC0xBmHmarXbDZldl+v18VB\n5+fnY6fcZoph3Ki1luthGMo7rmaqURTOCJY/g/y77v309FRCMooiCUlTGfMvBc/NzfFfvbrrQ9IS\nZxxmxsv9fl+2QO12W5K2UkpCbjAYyEmQ2WsOh8Ox/ePJyQkAW1tbha7TGcFMhXv79q0ccCRJImOa\nOI7lXHFtbU3u39zcBODdu3c8e/YMyDbfJndprQt9oc6HpCXO/WOD63iHWeIFs8QLZokXzBIvmCVe\nMEu8YJZ4wSzxglniBbPEC2aJF8wSL5glXjBLvGCWeMEs8YJZ4gWzxAtmiRfMEi+YJV4wS7xglvwG\nPYRJCWV343wAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7efd9db96a58>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}